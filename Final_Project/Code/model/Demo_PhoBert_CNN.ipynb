{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPZILmUBACMF",
        "outputId": "4434fc33-9f2d-4814-cff3-7f56b1ee34cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/7.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/7.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece -qq\n",
        "!pip install tokenizer -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3x2XGEhBi_A",
        "outputId": "d7472b7c-1099-44ee-d08b-04796c8c29a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build class CNN\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, n_filters, filter_sizes, output_dim,\n",
        "                 dropout, pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_input = nn.Linear(embedding_dim,embedding_dim)\n",
        "\n",
        "        self.conv_0 = nn.Conv1d(in_channels = embedding_dim,\n",
        "                                out_channels = n_filters,\n",
        "                                kernel_size = filter_sizes[0])\n",
        "\n",
        "        self.conv_1 = nn.Conv1d(in_channels = embedding_dim,\n",
        "                                out_channels = n_filters,\n",
        "                                kernel_size = filter_sizes[1])\n",
        "\n",
        "        self.conv_2 = nn.Conv1d(in_channels = embedding_dim,\n",
        "                                out_channels = n_filters,\n",
        "                                kernel_size = filter_sizes[2])\n",
        "\n",
        "        self.conv_3 = nn.Conv1d(in_channels = embedding_dim,\n",
        "                                out_channels = n_filters,\n",
        "                                kernel_size = filter_sizes[3])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = self.fc_input(encoded)\n",
        "        #print(embedded.shape)\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        #print(embedded.shape)\n",
        "\n",
        "        #embedded = [batch size, emb dim, sent len]\n",
        "\n",
        "        conved_0 = F.relu(self.conv_0(embedded))\n",
        "        conved_1 = F.relu(self.conv_1(embedded))\n",
        "        conved_2 = F.relu(self.conv_2(embedded))\n",
        "        conved_3 = F.relu(self.conv_3(embedded))\n",
        "\n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        pooled_3 = F.max_pool1d(conved_3, conved_3.shape[2]).squeeze(2)\n",
        "\n",
        "        #pooled_n = [batch size, n_fibatlters]\n",
        "\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2, pooled_3), dim = 1).cuda())\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        result =  self.fc(cat)\n",
        "\n",
        "        #print(result.shape)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "wgoabEhYB_at"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load phobert and cnn\n",
        "\n",
        "import torch\n",
        "phoBert = torch.load(r'/content/drive/MyDrive/Google Colab TMP/Bản sao của phobert_cnn_model_part1_task2a_2.pt')\n",
        "cnn = torch.load(r'/content/drive/MyDrive/Google Colab TMP/Bản sao của phobert_cnn_model_part2_task2a_2.pt')\n",
        "phoBert.eval()\n",
        "cnn.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAEPsdNaXMjm",
        "outputId": "7d362a61-521d-409f-fa66-676620733233"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (fc_input): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (conv_0): Conv1d(768, 32, kernel_size=(1,), stride=(1,))\n",
              "  (conv_1): Conv1d(768, 32, kernel_size=(2,), stride=(1,))\n",
              "  (conv_2): Conv1d(768, 32, kernel_size=(3,), stride=(1,))\n",
              "  (conv_3): Conv1d(768, 32, kernel_size=(5,), stride=(1,))\n",
              "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_html(txt):\n",
        "    return re.sub(r\"http\\S+\", \"\", txt)"
      ],
      "metadata": {
        "id": "6Wg7Uwp2Frox"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Start section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\n",
        "    Ví dụ: thủy = thuyr, tượng = tuwowngj\n",
        "\"\"\"\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "\n",
        "def vn_word_to_telex_type(word):\n",
        "    dau_cau = 0\n",
        "    new_word = ''\n",
        "    for char in word:\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            new_word += char\n",
        "            continue\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "        new_word += bang_nguyen_am[x][-1]\n",
        "    new_word += bang_ky_tu_dau[dau_cau]\n",
        "    return new_word\n",
        "\n",
        "\n",
        "def vn_sentence_to_telex_type(sentence):\n",
        "    \"\"\"\n",
        "    Chuyển câu tiếng việt có dấu về kiểu gõ telex.\n",
        "    :param sentence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        words[index] = vn_word_to_telex_type(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    End section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Co9WUz-wKgZV",
        "outputId": "36cf806c-7483-4c6c-b716-8dea03aa4cc1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    End section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "vn_word_to_telex_type('gà')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FKGO1fy6LJKz",
        "outputId": "b3533941-0253-47a0-e60f-b3ea1ffd401d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gaf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import regex as re\n",
        "\n",
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "\n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n"
      ],
      "metadata": {
        "id": "b1xzVkC-EgXN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "    Start section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
        "    Xem tại đây:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    End section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
        "    Xem tại đây: https://vi.wikipedia.org/wiki/Quy_tắc_đặt_dấu_thanh_trong_chữ_quốc_ngữ\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    print(chuan_hoa_dau_cau_tieng_viet('anh hoà, đang làm.. gì'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtNq886fF11_",
        "outputId": "5afa2b39-0abd-4130-91ab-33adc80b515c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anh hòa, đang làm.. gì\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lấy dữ liệu cho teencode\n",
        "import pandas as pd\n",
        "teencode_df = pd.read_csv('/content/drive/MyDrive/Google Colab TMP/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
        "teencode_list = teencode_df['teencode'].to_list()\n",
        "map_list = teencode_df['map'].to_list()"
      ],
      "metadata": {
        "id": "Nn9zBtb1GTVg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nltk for connect sent\n",
        "import nltk\n",
        "nltk.download('perluniprops')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcK2IywkICtm",
        "outputId": "1a105082-d7ac-476d-932e-548d1616e69e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses -qq"
      ],
      "metadata": {
        "id": "EIlvgdvPIERq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = ['Story', ':', 'I', 'wish', 'my', 'dog', \"'s\", 'hair', 'was', 'fluffier', ',', 'and', 'he', 'ate', 'better']\n",
        "\n",
        "#from nltk.tokenize.moses import MosesDetokenizer\n",
        "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
        "detokens = MosesDetokenizer().detokenize(test, return_str=True)\n",
        "detokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "63lhN3uGI4ua",
        "outputId": "58b8cd50-0e86-4d6c-f1ca-86da444bfe1c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Story: I wish my dog's hair was fluffier, and he ate better\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For chec a word is an eng word\n",
        "!apt install enchant\n",
        "!pip3 install pyenchant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbjW0cjdI6oZ",
        "outputId": "1fe859a4-3aa7-467d-c7f2-5ce43dbd0b53"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.7-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.7-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 1,316 kB of archives.\n",
            "After this operation, 5,474 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libtext-iconv-perl amd64 1.7-7 [13.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libaspell15 amd64 0.60.8-1ubuntu0.1 [328 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 emacsen-common all 3.0.4 [14.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 dictionaries-common all 1.28.1 [178 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 aspell amd64 0.60.8-1ubuntu0.1 [88.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 hunspell-en-us all 1:2018.04.16-1 [170 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libhunspell-1.7-0 amd64 1.7.0-2build2 [147 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 libenchant1c2a amd64 1.6.0-11.3build1 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 enchant amd64 1.6.0-11.3build1 [12.4 kB]\n",
            "Fetched 1,316 kB in 1s (1,184 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-7_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.8-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_3.0.4_all.deb ...\n",
            "Unpacking emacsen-common (3.0.4) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.28.1_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.1) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.8-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.8-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2018.04.16-0-1_all.deb ...\n",
            "Unpacking aspell-en (2018.04.16-0-1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2018.04.16-1_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2018.04.16-1) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.7-0_1.7.0-2build2_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.3build1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.3build1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.3build1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.3build1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7) ...\n",
            "Setting up libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\n",
            "Setting up emacsen-common (3.0.4) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\n",
            "Setting up dictionaries-common (1.28.1) ...\n",
            "Setting up aspell (0.60.8-1ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2018.04.16-1) ...\n",
            "Setting up aspell-en (2018.04.16-0-1) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.3build1) ...\n",
            "Setting up enchant (1.6.0-11.3build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for dictionaries-common (1.28.1) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import enchant\n",
        "eng = enchant.Dict(\"en_US\")"
      ],
      "metadata": {
        "id": "cNcARFBnJDBY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'hello, cute vcllll tộc ttrưởng đi đâu chơi đấyyyyyy :)))))))) ??????. Đc cc, được luôn. Hahahahahahaaaaaaaaaaaaaaaa ?'\n",
        "\n",
        "def remove_dub_char(sentence):\n",
        "  sentence = str(sentence)\n",
        "  words = []\n",
        "  for word in sentence.strip().split():\n",
        "    if word in teencode_list:\n",
        "      words.append(word)\n",
        "      continue\n",
        "    if eng.check(str(word)):\n",
        "      words.append(word)\n",
        "      continue\n",
        "    words.append(re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE))\n",
        "  return ' '.join(words)\n",
        "\n",
        "def remove_dub_spec_char(sentence):\n",
        "    sentence = str(sentence)\n",
        "    words = []\n",
        "    for word in sentence.strip().split():\n",
        "        if word in teencode_list:\n",
        "            words.append(word)\n",
        "            continue\n",
        "        if eng.check(str(word)):\n",
        "            words.append(word)\n",
        "            continue\n",
        "        cleaned_word = re.sub(r'([^\\w\\s])\\1+', r'\\1', word)\n",
        "        words.append(cleaned_word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "  # #Tokenize\n",
        "  # tokens_list = word_tokenize(sentence)\n",
        "  # for idx, word in enumerate(tokens_list):\n",
        "  #   if word in teencode_list:\n",
        "  #     continue\n",
        "  #   if eng.check(str(word)):\n",
        "  #     continue\n",
        "  #   worded = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n",
        "  #   if worded != word:\n",
        "  #     noneed_count += 1\n",
        "  #   tokens_list[idx] = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n",
        "\n",
        "  # tokens_list = word_tokenize()\n",
        "  # nestList_tokens = rdrsegmenter.tokenize(sentence)\n",
        "  # for tokens_idx, text_tokens in enumerate(nestList_tokens):\n",
        "  #   lenn += len(text_tokens)\n",
        "  #   for idx, word in enumerate(text_tokens):\n",
        "  #     #Neu tu co trong danh sach teencode thi continue\n",
        "  #     if word in teencode_list:\n",
        "  #       continue\n",
        "  #     if eng.check(str(word)):\n",
        "  #       continue\n",
        "  #     text_tokens[idx] = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n",
        "  #   nestList_tokens[tokens_idx] = text_tokens\n",
        "\n",
        "  # MAKE LIST FLAT\n",
        "  # flat_list = [item for sublist in tokens_list for item in sublist]\n",
        "\n",
        "  # detokens = MosesDetokenizer().detokenize(tokens_list, return_str=True)\n",
        "\n",
        "  # return detokens\n",
        "\n",
        "remove_dub_char(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5R28HqUJH9YF",
        "outputId": "31566091-57b8-4ec9-c01d-c25a3d29adad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'helo, cute vcl tộc trưởng đi đâu chơi đấy :)))))))) ??????. Đc c, được luôn. Hahahahahaha ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(document):\n",
        "    global link_count\n",
        "    global unicode_count\n",
        "    global dau_count\n",
        "    global lower_count\n",
        "    global noneed_count\n",
        "    global space_count\n",
        "\n",
        "    document = str(document)\n",
        "\n",
        "    # đưa về lower\n",
        "    ducument_before_lower = document\n",
        "    document = document.lower()\n",
        "    if ducument_before_lower != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_lower)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      lower_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # xóa khoảng trắng thừa\n",
        "    ducument_before_space = document\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    if ducument_before_space != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_space)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      space_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # xóa html code\n",
        "    ducument_before_html = document\n",
        "    document = remove_html(document)\n",
        "    if ducument_before_html != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_html)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      link_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # chuẩn hóa unicode\n",
        "    ducument_before_unicode = document\n",
        "    document = convert_unicode(document)\n",
        "    if ducument_before_unicode != document:\n",
        "      unicode_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # xóa các ký tự không cần thiết\n",
        "    ducument_before_redundant = document\n",
        "    document = remove_dub_char(document)\n",
        "    if ducument_before_redundant != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_redundant)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      noneed_count += 1\n",
        "    # del document1\n",
        "\n",
        "\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    ducument_before_dau = document\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    if ducument_before_dau != document:\n",
        "      print(\"Cau chua xu ly:\", ducument_before_dau)\n",
        "      print(\"Cau da xu ly:\", document)\n",
        "      dau_count += 1\n",
        "    # del document1\n",
        "\n",
        "    return document"
      ],
      "metadata": {
        "id": "wGJUq0IyGpJm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocess('hiếu THUẬN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EstgoANKJHzN",
        "outputId": "0c6e0b57-8bae-4496-e744-3e5a3bab82cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hiếu thuận'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacymoji -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxTHPRXOMSJ6",
        "outputId": "50093dd1-d198-4e29-d77e-6771fdd372cc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/356.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m348.2/356.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.6/356.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67fe8-TzMUva",
        "outputId": "7f7ccd91-a6a9-4195-e828-b2c6b6a073ab"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-05 03:56:03.745102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '       tiên tri nói         quằn qùeeeee hay vcllll :))       ) '\n",
        "phase1_text = text_preprocess(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IFVGSEtNbk-",
        "outputId": "8bae5d91-4713-4cfe-d423-9d5c8f03aab6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cau chua xu ly: tiên tri nói quằn qùe hay vcl :)) )\n",
            "Cau da xu ly: tiên tri nói quằn què hay vcl :)) )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phase1_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0U4lGz6gNwDy",
        "outputId": "8a83e235-e037-4110-ffdc-68890da9793c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiên tri nói quằn què hay vcl :)) )'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word segmenter\n",
        "!pip3 install vncorenlp\n",
        "\n",
        "# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK6zBB79NxYM",
        "outputId": "b8521cb7-160e-4eed-fc0b-1c23b9a68c92"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=a6cab630113b3abe3d53c499f89fe10516ee5195b58cb80f93c4a73e186bacea\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "--2023-07-05 04:02:52--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   169MB/s    in 0.2s    \n",
            "\n",
            "2023-07-05 04:02:53 (169 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2023-07-05 04:02:53--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-05 04:02:53 (14.0 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2023-07-05 04:02:53--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-05 04:02:53 (6.03 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "\n",
        "text = \"hello, hòa thuận\"\n",
        "\n",
        "word_segmented_text = rdrsegmenter.tokenize(text)\n",
        "print(type(word_segmented_text))\n",
        "print(word_segmented_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K75zklrvN7qc",
        "outputId": "723f3cf1-31f0-4db5-d534-cd72614baad0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "[['hello', ',', 'hoà_thuận']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def searchTeencode(word):\n",
        "  try:\n",
        "    global teencode_count\n",
        "    index = teencode_list.index(word)\n",
        "    map_word = map_list[index]\n",
        "    teencode_count += 1\n",
        "    return map_word\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "UG4BlXTQN_q_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopword = ['bị', 'bởi', 'cả', 'các', 'cái', 'cần', 'càng', 'chỉ', 'chiếc', 'cho', 'chứ', 'chưa', 'chuyện', 'có', 'có_thể', 'cứ',\n",
        "#             'của', 'cùng', 'cũng', 'đã', 'đang', 'đây', 'để', 'đến_nỗi', 'đều', 'điều', 'do', 'đó', 'được', 'dưới', 'gì', 'khi',\n",
        "#             'không', 'là', 'lại', 'lên', 'lúc', 'mà', 'mỗi', 'một_cách', 'này', 'nên', 'nếu', 'ngay', 'nhiều', 'như', 'nhưng',\n",
        "#             'những', 'nơi', 'nữa', 'phải', 'qua', 'ra', 'rằng', 'rất', 'rồi', 'sau', 'sẽ', 'so', 'sự', 'tại', 'theo', 'thì', 'trên',\n",
        "#             'trước', 'từ', 'từng', 'và', 'vẫn', 'vào', 'vậy', 'vì', 'việc', 'với', 'vừa']\n",
        "\n",
        "STOPWORDS = '/content/drive/MyDrive/Google Colab TMP/vietnamese-stopwords-dash.txt'\n",
        "\n",
        "# features extraction\n",
        "with open(STOPWORDS, \"r\") as ins:\n",
        "    stopword = []\n",
        "    for line in ins:\n",
        "        stopword.append(line.strip('\\n'))\n",
        "\n",
        "def remove_stopwords(line):\n",
        "    global stopword_count\n",
        "    words = []\n",
        "    for word in line:\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "        if word in stopword:\n",
        "            stopword_count += 1\n",
        "    return words\n",
        "\n",
        "# with open(STOPWORDS, \"r\") as ins:\n",
        "#     stop_words = []\n",
        "#     for line in ins:\n",
        "#         stop_words.append(line.strip('\\n'))"
      ],
      "metadata": {
        "id": "ENJXcVpbOEPL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DE STOPWORD_TEENCODE BY VNCORENLP\n",
        "stopword_count = 0\n",
        "teencode_count =0\n",
        "def stopWords_Teencode(sentence):\n",
        "  lenn = 0\n",
        "  sentence = str(sentence)\n",
        "  #Tokenize\n",
        "  nestList_tokens = rdrsegmenter.tokenize(sentence)\n",
        "  for tokens_idx, text_tokens in enumerate(nestList_tokens):\n",
        "    #Teencode\n",
        "    lenn += len(text_tokens)\n",
        "    for idx, word in enumerate(text_tokens):\n",
        "      deteencoded = searchTeencode(word)\n",
        "      if(deteencoded != None):\n",
        "        text_tokens[idx] = deteencoded\n",
        "    nestList_tokens[tokens_idx] = text_tokens\n",
        "\n",
        "  # deteencode_sentence = (\" \").join(nestList_tokens)\n",
        "\n",
        "  #Stopwords\n",
        "  flat_list = [item for sublist in nestList_tokens for item in sublist]\n",
        "\n",
        "  List_without_sw = remove_stopwords(flat_list)\n",
        "\n",
        "  #Detokenize\n",
        "  detokens = MosesDetokenizer().detokenize(List_without_sw, return_str=True)\n",
        "\n",
        "  return detokens"
      ],
      "metadata": {
        "id": "px50qI0eOmF-"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'đm điên thật đéo biết phải làm gì luôn á'"
      ],
      "metadata": {
        "id": "QpvipWLJOp_6"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopWords_Teencode(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7GRkB5neOtgV",
        "outputId": "bcdb4203-16b8-4c07-d464-f9e4810b435b"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'địt mẹ điên đéo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_input = stopWords_Teencode(sent)"
      ],
      "metadata": {
        "id": "b3lsgK0yRhQt"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AemQTz8pRj4N",
        "outputId": "ca1f30fb-c33c-4746-f871-ece9d0fc615a"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'địt mẹ điên đéo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C20PfIkO6vp",
        "outputId": "9addb841-cff7-4d76-ac49-248d3d8ff96f"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "encoded_dict = tokenizer.encode_plus(sent_input,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=50,\n",
        "                                             pad_to_max_length=True,\n",
        "                                             truncation = True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_tensors='pt')\n",
        "input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "metadata": {
        "id": "6mjbwHxhPh9A"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.cat(input_ids,dim=0).cuda()\n",
        "attention_masks = torch.cat(attention_masks,dim=0).cuda()"
      ],
      "metadata": {
        "id": "DuEE9r_JQXHk"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded = phoBert(input_ids,attention_masks)[0]"
      ],
      "metadata": {
        "id": "fy11yNIMQCtq"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = cnn(embedded)"
      ],
      "metadata": {
        "id": "pv39CAuWQbVx"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " predictions = predictions.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "b1zNnEK9QgWb"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhNebgzUSBYK",
        "outputId": "08e09ba2-685d-40a9-f2df-645894dfe279"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.383103 ,  5.0419154, -3.3497438]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "prediction = np.argmax(predictions)\n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqE4pUCaQlmI",
        "outputId": "5494821d-ef7e-4bf7-cc1d-5abf976e46e2"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if prediction == 0:\n",
        "  label = \"clean\"\n",
        "elif prediction == 1:\n",
        "  label = 'offensive'\n",
        "else:\n",
        "  label = \"hate\""
      ],
      "metadata": {
        "id": "CdGy77-tRMiN"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cau ban nhap la:\" + sent + \"\\n\")\n",
        "print(\"Ket qua la:\" + label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0DNADpPRDTs",
        "outputId": "9a54f06a-43ba-419e-c1fe-d394ad4661c9"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cau ban nhap la:thằng đó đẹp trai vl, đéo thể tin được nhưng mà còn ngu nữa, haha. Con cặc, đjt mẹ, thg đàn bà bê đê chúa\n",
            "\n",
            "Ket qua la:offensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def predict(sent_input):\n",
        "  sent_input = stopWords_Teencode(sent_input)\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  encoded_dict = tokenizer.encode_plus(sent_input,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=50,\n",
        "                                             padding='longest',\n",
        "                                             truncation = True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_tensors='pt')\n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  input_ids = torch.cat(input_ids,dim=0).cuda()\n",
        "  attention_masks = torch.cat(attention_masks,dim=0).cuda()\n",
        "\n",
        "  embedded = phoBert(input_ids,attention_masks)[0]\n",
        "  predictions = cnn(embedded)\n",
        "  predictions = predictions.detach().cpu().numpy()\n",
        "  prediction = np.argmax(predictions)\n",
        "  if prediction == 0:\n",
        "    label = \"clean\"\n",
        "  elif prediction == 1:\n",
        "    label = 'offensive'\n",
        "  else:\n",
        "    label = \"hate\"\n",
        "\n",
        "  print(\"Cau ban nhap la:\" + sent + \"\\n\")\n",
        "  print(\"Ket qua la:\" + label)"
      ],
      "metadata": {
        "id": "pqoFpExNUwpq"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"Xe Việt Nam vẫn quá đắt và đường xá thì quá kém\"\n",
        "predict(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZA4eJ4QVIJO",
        "outputId": "497a7676-7add-42de-c05f-2e274cb6f0d0"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cau ban nhap la:Xe Việt Nam vẫn quá đắt và đường xá thì quá kém\n",
            "\n",
            "Ket qua la:clean\n"
          ]
        }
      ]
    }
  ]
}