{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VIF50b-oczf6GVQB-wSTYD48Mb2cSTFs","timestamp":1688406331213}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgdkjVf_oB8j","outputId":"5cbb67cd-3f72-4de4-b0bb-e01ac86bbe77","executionInfo":{"status":"ok","timestamp":1688464810300,"user_tz":-420,"elapsed":25664,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"LaYxfGHveC2e"},"source":["## Import data"]},{"cell_type":"code","source":["!git clone https://github.com/IrisPham74/CS117ToxicComment.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVYGV0Q28Vfe","executionInfo":{"status":"ok","timestamp":1688464814093,"user_tz":-420,"elapsed":1835,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"3eb8a457-a31c-49be-cc35-3a31fc6485ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CS117ToxicComment'...\n","remote: Enumerating objects: 16, done.\u001b[K\n","remote: Counting objects: 100% (16/16), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 16 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (16/16), 317.11 KiB | 1.62 MiB/s, done.\n"]}]},{"cell_type":"code","metadata":{"id":"2tmjbD8IWI0u"},"source":["import pandas as pd\n","datafolderPath = \"/content/CS117ToxicComment/Dataset.xlsx\"\n","\n","#dev = pd.read_csv(datafolderPath+\"/dev.csv\")\n","#test = pd.read_csv(datafolderPath+\"/test.csv\")\n","dataset = pd.read_excel(datafolderPath)\n","\n","#dataset = pd.concat([train, dev, test], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tavb8gacc4eK","colab":{"base_uri":"https://localhost:8080/","height":423},"outputId":"24ba3744-3430-4421-ab7d-bd9eb2aec0e2","executionInfo":{"status":"ok","timestamp":1688464818919,"user_tz":-420,"elapsed":483,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       No.                 Comment  Label\n","0        1                  ƒëu v l      1\n","1        2  m·∫•y a Favelas nh·ªìng vl      1\n","2        3    Li√™n hoan x√°c th·ªãt:v      0\n","3        4       C√° r√¥ phi gh√™ vcl      1\n","4        5          nip tay to vcl      1\n","...    ...                     ...    ...\n","8088  8089  ruler b·∫Øn ch√°y v√£i l·ªìn      1\n","8089  8090     v√£i th·∫≠t m·ªõi 1 game      1\n","8090  8091   999 lam may qua ac vl      1\n","8091  8092                   ƒë√¢u r      0\n","8092  8093                  T2 cay      0\n","\n","[8093 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-66384ed6-97c6-4079-9767-87ed0e98a5a9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No.</th>\n","      <th>Comment</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>ƒëu v l</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>m·∫•y a Favelas nh·ªìng vl</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Li√™n hoan x√°c th·ªãt:v</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>C√° r√¥ phi gh√™ vcl</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>nip tay to vcl</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8088</th>\n","      <td>8089</td>\n","      <td>ruler b·∫Øn ch√°y v√£i l·ªìn</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8089</th>\n","      <td>8090</td>\n","      <td>v√£i th·∫≠t m·ªõi 1 game</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8090</th>\n","      <td>8091</td>\n","      <td>999 lam may qua ac vl</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8091</th>\n","      <td>8092</td>\n","      <td>ƒë√¢u r</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8092</th>\n","      <td>8093</td>\n","      <td>T2 cay</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8093 rows √ó 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66384ed6-97c6-4079-9767-87ed0e98a5a9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-66384ed6-97c6-4079-9767-87ed0e98a5a9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-66384ed6-97c6-4079-9767-87ed0e98a5a9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"VmvowkcWV49I"},"source":["#ULTIMATE"]},{"cell_type":"code","metadata":{"id":"VNgDTB69Odxj"},"source":["import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JCoTBsdNRBIu"},"source":["Xo√° HTML"]},{"cell_type":"code","metadata":{"id":"AZRkT0txOPzz"},"source":["def remove_html(txt):\n","    return re.sub(r\"http\\S+\", \"\", txt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otueGdYQUDYV"},"source":["##Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey"]},{"cell_type":"code","metadata":{"id":"8x5-4pmiUEjk","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"b0fcc63a-ad17-4fcb-ce53-b51dfec5e13d","executionInfo":{"status":"ok","timestamp":1688464827666,"user_tz":-420,"elapsed":5,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["\"\"\"\n","    Start section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey\n","    V√≠ d·ª•: th·ªßy = thuyr, t∆∞·ª£ng = tuwowngj\n","\"\"\"\n","bang_nguyen_am = [['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],\n","                  ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],\n","                  ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],\n","                  ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e'],\n","                  ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],\n","                  ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i'],\n","                  ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o'],\n","                  ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],\n","                  ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],\n","                  ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u'],\n","                  ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],\n","                  ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']]\n","bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n","\n","nguyen_am_to_ids = {}\n","\n","for i in range(len(bang_nguyen_am)):\n","    for j in range(len(bang_nguyen_am[i]) - 1):\n","        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n","\n","\n","def vn_word_to_telex_type(word):\n","    dau_cau = 0\n","    new_word = ''\n","    for char in word:\n","        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n","        if x == -1:\n","            new_word += char\n","            continue\n","        if y != 0:\n","            dau_cau = y\n","        new_word += bang_nguyen_am[x][-1]\n","    new_word += bang_ky_tu_dau[dau_cau]\n","    return new_word\n","\n","\n","def vn_sentence_to_telex_type(sentence):\n","    \"\"\"\n","    Chuy·ªÉn c√¢u ti·∫øng vi·ªát c√≥ d·∫•u v·ªÅ ki·ªÉu g√µ telex.\n","    :param sentence:\n","    :return:\n","    \"\"\"\n","    words = sentence.split()\n","    for index, word in enumerate(words):\n","        words[index] = vn_word_to_telex_type(word)\n","    return ' '.join(words)\n","\n","\n","\"\"\"\n","    End section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n    End section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#Testing\n","vn_word_to_telex_type('g√†')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"R8fRLySjpbAd","outputId":"907d8e24-bc6b-4894-c76a-51a6069aff35","executionInfo":{"status":"ok","timestamp":1688464830838,"user_tz":-420,"elapsed":517,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gaf'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"eNG-qc3tRF3Z"},"source":["# **Chu·∫©n ho√° unicode**"]},{"cell_type":"code","metadata":{"id":"vr_EHfVoRUex"},"source":["# -*- coding: utf-8 -*-\n","\n","import regex as re\n","\n","uniChars = \"√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø\"\n","unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n","\n","\n","def loaddicchar():\n","    dic = {}\n","    char1252 = '√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'.split(\n","        '|')\n","    charutf8 = \"√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥\".split(\n","        '|')\n","    for i in range(len(char1252)):\n","        dic[char1252[i]] = charutf8[i]\n","    return dic\n","\n","\n","dicchar = loaddicchar()\n","\n","\n","def convert_unicode(txt):\n","    return re.sub(\n","        r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥',\n","        lambda x: dicchar[x.group()], txt)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8BMFB0kUNsu"},"source":["D√πng √≤a √∫y thay o√† u√Ω"]},{"cell_type":"code","metadata":{"id":"yjJPkrKgUOwH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d66f7c0-32a4-4a64-dc90-a46a23c8140d","executionInfo":{"status":"ok","timestamp":1688464837005,"user_tz":-420,"elapsed":2,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["\n","\"\"\"\n","    Start section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ c√°ch g√µ d·∫•u ki·ªÉu c≈©: d√πng √≤a √∫y thay o√† u√Ω\n","    Xem t·∫°i ƒë√¢y:\n","\n","\"\"\"\n","\n","\n","def chuan_hoa_dau_tu_tieng_viet(word):\n","    if not is_valid_vietnam_word(word):\n","        return word\n","\n","    chars = list(word)\n","    dau_cau = 0\n","    nguyen_am_index = []\n","    qu_or_gi = False\n","    for index, char in enumerate(chars):\n","        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n","        if x == -1:\n","            continue\n","        elif x == 9:  # check qu\n","            if index != 0 and chars[index - 1] == 'q':\n","                chars[index] = 'u'\n","                qu_or_gi = True\n","        elif x == 5:  # check gi\n","            if index != 0 and chars[index - 1] == 'g':\n","                chars[index] = 'i'\n","                qu_or_gi = True\n","        if y != 0:\n","            dau_cau = y\n","            chars[index] = bang_nguyen_am[x][0]\n","        if not qu_or_gi or index != 1:\n","            nguyen_am_index.append(index)\n","    if len(nguyen_am_index) < 2:\n","        if qu_or_gi:\n","            if len(chars) == 2:\n","                x, y = nguyen_am_to_ids.get(chars[1])\n","                chars[1] = bang_nguyen_am[x][dau_cau]\n","            else:\n","                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n","                if x != -1:\n","                    chars[2] = bang_nguyen_am[x][dau_cau]\n","                else:\n","                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n","            return ''.join(chars)\n","        return word\n","\n","    for index in nguyen_am_index:\n","        x, y = nguyen_am_to_ids[chars[index]]\n","        if x == 4 or x == 8:  # √™, ∆°\n","            chars[index] = bang_nguyen_am[x][dau_cau]\n","            # for index2 in nguyen_am_index:\n","            #     if index2 != index:\n","            #         x, y = nguyen_am_to_ids[chars[index]]\n","            #         chars[index2] = bang_nguyen_am[x][0]\n","            return ''.join(chars)\n","\n","    if len(nguyen_am_index) == 2:\n","        if nguyen_am_index[-1] == len(chars) - 1:\n","            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n","            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n","            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n","            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n","        else:\n","            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n","            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n","            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n","            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n","    else:\n","        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n","        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n","        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n","        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n","        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n","        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n","    return ''.join(chars)\n","\n","\n","def is_valid_vietnam_word(word):\n","    chars = list(word)\n","    nguyen_am_index = -1\n","    for index, char in enumerate(chars):\n","        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n","        if x != -1:\n","            if nguyen_am_index == -1:\n","                nguyen_am_index = index\n","            else:\n","                if index - nguyen_am_index != 1:\n","                    return False\n","                nguyen_am_index = index\n","    return True\n","\n","\n","def chuan_hoa_dau_cau_tieng_viet(sentence):\n","    \"\"\"\n","        Chuy·ªÉn c√¢u ti·∫øng vi·ªát v·ªÅ chu·∫©n g√µ d·∫•u ki·ªÉu c≈©.\n","        :param sentence:\n","        :return:\n","        \"\"\"\n","    sentence = sentence.lower()\n","    words = sentence.split()\n","    for index, word in enumerate(words):\n","        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n","        # print(cw)\n","        if len(cw) == 3:\n","            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n","        words[index] = ''.join(cw)\n","    return ' '.join(words)\n","\n","\n","\"\"\"\n","    End section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ c√°ch g√µ d·∫•u ki·ªÉu c≈©: d√πng √≤a √∫y thay o√† u√Ω\n","    Xem t·∫°i ƒë√¢y: https://vi.wikipedia.org/wiki/Quy_t·∫Øc_ƒë·∫∑t_d·∫•u_thanh_trong_ch·ªØ_qu·ªëc_ng·ªØ\n","\"\"\"\n","if __name__ == '__main__':\n","    print(chuan_hoa_dau_cau_tieng_viet('anh ho√†, ƒëang l√†m.. g√¨'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["anh h√≤a, ƒëang l√†m.. g√¨\n"]}]},{"cell_type":"markdown","metadata":{"id":"k7gXpXMuVnY_"},"source":["Lo·∫°i b·ªè k√Ω t·ª± c·ªë t√≠nh vi·∫øt d√†i trong c√¢u:\n","\n","eg: ƒë√¢yyyyyyyyyy -> ƒë√¢y"]},{"cell_type":"code","metadata":{"id":"qOKJRUGXXiwU"},"source":["# l·∫•y d·ªØ li·ªáu cho teencode\n","import pandas as pd\n","teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)\n","teencode_list = teencode_df['teencode'].to_list()\n","map_list = teencode_df['map'].to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzP1ndUSOiyt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8651a730-99b2-4529-a6f9-ec60f916f93f","executionInfo":{"status":"ok","timestamp":1688464845550,"user_tz":-420,"elapsed":2509,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["# Nltk for connect sent\n","import nltk\n","nltk.download('perluniprops')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package perluniprops to /root/nltk_data...\n","[nltk_data]   Unzipping misc/perluniprops.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["!pip install sacremoses"],"metadata":{"id":"UqHtjTzw-P3I","executionInfo":{"status":"ok","timestamp":1688464852344,"user_tz":-420,"elapsed":6796,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c0d3fd5-b458-4a7a-af33-179c161e088a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=2144176336516680137b78ef5f6ee248ef9f0efff75a977820b495882c5ca13c\n","  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n","Successfully built sacremoses\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.0.53\n"]}]},{"cell_type":"code","metadata":{"id":"xX5-Iyl3Ou21","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"1296ae31-16ce-4d11-9294-38fbcfcee563","executionInfo":{"status":"ok","timestamp":1688464855340,"user_tz":-420,"elapsed":9,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["test = ['Story', ':', 'I', 'wish', 'my', 'dog', \"'s\", 'hair', 'was', 'fluffier', ',', 'and', 'he', 'ate', 'better']\n","\n","#from nltk.tokenize.moses import MosesDetokenizer\n","from sacremoses import MosesTokenizer, MosesDetokenizer\n","detokens = MosesDetokenizer().detokenize(test, return_str=True)\n","detokens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Story: I wish my dog's hair was fluffier, and he ate better\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# For chec a word is an eng word\n","!apt install enchant\n","!pip3 install pyenchant"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjE3lUbq7PZk","executionInfo":{"status":"ok","timestamp":1688464886336,"user_tz":-420,"elapsed":27868,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"e02f5d30-2c07-4396-8b81-c2d3085459e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.7-0 libtext-iconv-perl\n","Suggested packages:\n","  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n","  | openoffice.org-core libenchant-voikko\n","The following NEW packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.7-0 libtext-iconv-perl\n","0 upgraded, 10 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 1,316 kB of archives.\n","After this operation, 5,474 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libtext-iconv-perl amd64 1.7-7 [13.8 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libaspell15 amd64 0.60.8-1ubuntu0.1 [328 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 emacsen-common all 3.0.4 [14.9 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 dictionaries-common all 1.28.1 [178 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 aspell amd64 0.60.8-1ubuntu0.1 [88.4 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 hunspell-en-us all 1:2018.04.16-1 [170 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libhunspell-1.7-0 amd64 1.7.0-2build2 [147 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 libenchant1c2a amd64 1.6.0-11.3build1 [64.7 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 enchant amd64 1.6.0-11.3build1 [12.4 kB]\n","Fetched 1,316 kB in 2s (651 kB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package libtext-iconv-perl.\n","(Reading database ... 123069 files and directories currently installed.)\n","Preparing to unpack .../0-libtext-iconv-perl_1.7-7_amd64.deb ...\n","Unpacking libtext-iconv-perl (1.7-7) ...\n","Selecting previously unselected package libaspell15:amd64.\n","Preparing to unpack .../1-libaspell15_0.60.8-1ubuntu0.1_amd64.deb ...\n","Unpacking libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\n","Selecting previously unselected package emacsen-common.\n","Preparing to unpack .../2-emacsen-common_3.0.4_all.deb ...\n","Unpacking emacsen-common (3.0.4) ...\n","Selecting previously unselected package dictionaries-common.\n","Preparing to unpack .../3-dictionaries-common_1.28.1_all.deb ...\n","Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n","Unpacking dictionaries-common (1.28.1) ...\n","Selecting previously unselected package aspell.\n","Preparing to unpack .../4-aspell_0.60.8-1ubuntu0.1_amd64.deb ...\n","Unpacking aspell (0.60.8-1ubuntu0.1) ...\n","Selecting previously unselected package aspell-en.\n","Preparing to unpack .../5-aspell-en_2018.04.16-0-1_all.deb ...\n","Unpacking aspell-en (2018.04.16-0-1) ...\n","Selecting previously unselected package hunspell-en-us.\n","Preparing to unpack .../6-hunspell-en-us_1%3a2018.04.16-1_all.deb ...\n","Unpacking hunspell-en-us (1:2018.04.16-1) ...\n","Selecting previously unselected package libhunspell-1.7-0:amd64.\n","Preparing to unpack .../7-libhunspell-1.7-0_1.7.0-2build2_amd64.deb ...\n","Unpacking libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\n","Selecting previously unselected package libenchant1c2a:amd64.\n","Preparing to unpack .../8-libenchant1c2a_1.6.0-11.3build1_amd64.deb ...\n","Unpacking libenchant1c2a:amd64 (1.6.0-11.3build1) ...\n","Selecting previously unselected package enchant.\n","Preparing to unpack .../9-enchant_1.6.0-11.3build1_amd64.deb ...\n","Unpacking enchant (1.6.0-11.3build1) ...\n","Setting up libtext-iconv-perl (1.7-7) ...\n","Setting up libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\n","Setting up emacsen-common (3.0.4) ...\n","Setting up libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\n","Setting up dictionaries-common (1.28.1) ...\n","Setting up aspell (0.60.8-1ubuntu0.1) ...\n","Setting up hunspell-en-us (1:2018.04.16-1) ...\n","Setting up aspell-en (2018.04.16-0-1) ...\n","Setting up libenchant1c2a:amd64 (1.6.0-11.3build1) ...\n","Setting up enchant (1.6.0-11.3build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","Processing triggers for dictionaries-common (1.28.1) ...\n","aspell-autobuildhash: processing: en [en-common].\n","aspell-autobuildhash: processing: en [en-variant_0].\n","aspell-autobuildhash: processing: en [en-variant_1].\n","aspell-autobuildhash: processing: en [en-variant_2].\n","aspell-autobuildhash: processing: en [en-w_accents-only].\n","aspell-autobuildhash: processing: en [en-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-variant_0].\n","aspell-autobuildhash: processing: en [en_AU-variant_1].\n","aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-variant_0].\n","aspell-autobuildhash: processing: en [en_CA-variant_1].\n","aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-variant_0].\n","aspell-autobuildhash: processing: en [en_GB-variant_1].\n","aspell-autobuildhash: processing: en [en_US-w_accents-only].\n","aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n","Collecting pyenchant\n","  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyenchant\n","Successfully installed pyenchant-3.2.2\n"]}]},{"cell_type":"code","metadata":{"id":"O2gQHRZhvwcN"},"source":["import enchant\n","eng = enchant.Dict(\"en_US\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"GtchyroHIsAj","outputId":"4736292f-f9a4-4983-ab1a-574856d57439","executionInfo":{"status":"ok","timestamp":1688464991966,"user_tz":-420,"elapsed":442,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["sent = 'hello, cute vcllll t·ªôc ttr∆∞·ªüng ƒëi ƒë√¢u ch∆°i ƒë·∫•yyyyyy :)))))))) ??????. ƒêc cc, ƒë∆∞·ª£c lu√¥n. Hahahahahahaaaaaaaaaaaaaaaa ?'\n","\n","def remove_dub_char(sentence):\n","  sentence = str(sentence)\n","  words = []\n","  for word in sentence.strip().split():\n","    if word in teencode_list:\n","      words.append(word)\n","      continue\n","    if eng.check(str(word)):\n","      words.append(word)\n","      continue\n","    words.append(re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE))\n","  return ' '.join(words)\n","\n","def remove_dub_spec_char(sentence):\n","    sentence = str(sentence)\n","    words = []\n","    for word in sentence.strip().split():\n","        if word in teencode_list:\n","            words.append(word)\n","            continue\n","        if eng.check(str(word)):\n","            words.append(word)\n","            continue\n","        cleaned_word = re.sub(r'([^\\w\\s])\\1+', r'\\1', word)\n","        words.append(cleaned_word)\n","    return ' '.join(words)\n","\n","  # #Tokenize\n","  # tokens_list = word_tokenize(sentence)\n","  # for idx, word in enumerate(tokens_list):\n","  #   if word in teencode_list:\n","  #     continue\n","  #   if eng.check(str(word)):\n","  #     continue\n","  #   worded = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n","  #   if worded != word:\n","  #     noneed_count += 1\n","  #   tokens_list[idx] = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n","\n","  # tokens_list = word_tokenize()\n","  # nestList_tokens = rdrsegmenter.tokenize(sentence)\n","  # for tokens_idx, text_tokens in enumerate(nestList_tokens):\n","  #   lenn += len(text_tokens)\n","  #   for idx, word in enumerate(text_tokens):\n","  #     #Neu tu co trong danh sach teencode thi continue\n","  #     if word in teencode_list:\n","  #       continue\n","  #     if eng.check(str(word)):\n","  #       continue\n","  #     text_tokens[idx] = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n","  #   nestList_tokens[tokens_idx] = text_tokens\n","\n","  # MAKE LIST FLAT\n","  # flat_list = [item for sublist in tokens_list for item in sublist]\n","\n","  # detokens = MosesDetokenizer().detokenize(tokens_list, return_str=True)\n","\n","  # return detokens\n","\n","remove_dub_char(sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'helo, cute vcl t·ªôc tr∆∞·ªüng ƒëi ƒë√¢u ch∆°i ƒë·∫•y :)))))))) ??????. ƒêc c, ƒë∆∞·ª£c lu√¥n. Hahahahahaha ?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"9Wti4h-oUxSx"},"source":["**T√°ch t·ª´ ti·∫øng Vi·ªát**\n","\n","H·ªçc sinh h·ªçc sinh h·ªçc ‚áí H·ªçc_sinh h·ªçc sinh_h·ªçc"]},{"cell_type":"markdown","source":["# GIAI ƒêO·∫†N 1\n","1. lowercase sentences\n","2. delete redundant spaces\n","3. delete links\n","4. normalize unicode\n","5. delete redundant characters\n","6. normalize accented letters"],"metadata":{"id":"iTptNXUy7sEu"}},{"cell_type":"code","metadata":{"id":"R7hgkCT_vDpE"},"source":["link_count = 0\n","unicode_count = 0\n","dau_count = 0\n","lower_count = 0\n","noneed_count = 0\n","space_count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LOK9e25VuPX"},"source":["def text_preprocess(document):\n","    global link_count\n","    global unicode_count\n","    global dau_count\n","    global lower_count\n","    global noneed_count\n","    global space_count\n","\n","    document = str(document)\n","\n","    # ƒë∆∞a v·ªÅ lower\n","    ducument_before_lower = document\n","    document = document.lower()\n","    if ducument_before_lower != document:\n","      # print(\"Cau chua xu ly:\", ducument_before_lower)\n","      # print(\"Cau da xu ly:\", document)\n","      lower_count += 1\n","    # del document1\n","\n","    # x√≥a kho·∫£ng tr·∫Øng th·ª´a\n","    ducument_before_space = document\n","    document = re.sub(r'\\s+', ' ', document).strip()\n","    if ducument_before_space != document:\n","      # print(\"Cau chua xu ly:\", ducument_before_space)\n","      # print(\"Cau da xu ly:\", document)\n","      space_count += 1\n","    # del document1\n","\n","    # x√≥a html code\n","    ducument_before_html = document\n","    document = remove_html(document)\n","    if ducument_before_html != document:\n","      # print(\"Cau chua xu ly:\", ducument_before_html)\n","      # print(\"Cau da xu ly:\", document)\n","      link_count += 1\n","    # del document1\n","\n","    # chu·∫©n h√≥a unicode\n","    ducument_before_unicode = document\n","    document = convert_unicode(document)\n","    if ducument_before_unicode != document:\n","      unicode_count += 1\n","    # del document1\n","\n","    # x√≥a c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt\n","    ducument_before_redundant = document\n","    document = remove_dub_char(document)\n","    if ducument_before_redundant != document:\n","      # print(\"Cau chua xu ly:\", ducument_before_redundant)\n","      # print(\"Cau da xu ly:\", document)\n","      noneed_count += 1\n","    # del document1\n","\n","\n","    # chu·∫©n h√≥a c√°ch g√µ d·∫•u ti·∫øng Vi·ªát\n","    ducument_before_dau = document\n","    document = chuan_hoa_dau_cau_tieng_viet(document)\n","    if ducument_before_dau != document:\n","      print(\"Cau chua xu ly:\", ducument_before_dau)\n","      print(\"Cau da xu ly:\", document)\n","      dau_count += 1\n","    # del document1\n","\n","    return document"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EPBjgWJtts3t","outputId":"a5e7681a-d439-40f8-c137-8a7e42ffb66d","executionInfo":{"status":"ok","timestamp":1688454874842,"user_tz":-420,"elapsed":5,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["text_preprocess('Hi·∫øu THU·∫¨N')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'hi·∫øu thu·∫≠n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVIIBGL6uUVW","outputId":"89a083c1-0fcf-41d1-94ec-efa61dd7c503","executionInfo":{"status":"ok","timestamp":1688454875343,"user_tz":-420,"elapsed":2,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["'hi·∫øu thu·∫≠n' == 'Hi·∫øu THU·∫¨N'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"HYwwNVpAd2PE"},"source":["#DO IT"]},{"cell_type":"code","metadata":{"id":"u9Hs8uurt0vD"},"source":["import pandas as pd\n","datafolderPath = \"/content/CS117ToxicComment/Dataset.xlsx\"\n","\n","#dev = pd.read_csv(datafolderPath+\"/dev.csv\")\n","#test = pd.read_csv(datafolderPath+\"/test.csv\")\n","#train = pd.read_csv(datafolderPath+\"/train.csv\")\n","\n","#dataset = pd.concat([train, dev, test], ignore_index=True)\n","dataset = pd.read_excel(datafolderPath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wqr8Je3NVowX"},"source":["#VLSP\n","#import pandas as pd\n","#train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/HATE SPEECH SENTIMENT/DataFromVLSP/Copy of trainbert.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoT9UyarVxA-"},"source":["#VLSP\n","#kq = train\n","kq = dataset\n","x_train = kq.iloc[:, 1] #0 - 1\n","x_train = pd.DataFrame(x_train)\n","x_val = kq.iloc[:, 2] #1-2 #values\n","x_val = pd.DataFrame(x_val)\n","x_val_full = kq.iloc[:, 2:] #1-2 #values\n","x_val_full = pd.DataFrame(x_val_full)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Adfs_BbveIDZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"829970b7-89c9-4b81-d4a7-adb0248de8f4","executionInfo":{"status":"ok","timestamp":1688454896965,"user_tz":-420,"elapsed":3045,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["link_count = 0\n","unicode_count = 0\n","dau_count = 0\n","lower_count = 0\n","noneed_count = 0\n","space_count = 0\n","#train['free_text'] = train['free_text'].apply(lambda x:text_preprocess(x))\n","dataset['Comment'] = dataset['Comment'].apply(lambda x:text_preprocess(x))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cau chua xu ly: x·∫°o cho√° qu√° anh\n","Cau da xu ly: x·∫°o ch√≥a qu√° anh\n","Cau chua xu ly: cerberus l∆∞∆°ng cao nh·∫•t vi·ªát nam, nh∆∞ng so v·ªõi sin, indo th√¨ b·∫±ng 1/3 =))\n","Cau da xu ly: cerberus l∆∞∆°ng cao nh·∫•t vi·ªát nam, nh∆∞ng so v·ªõi sin, indo th√¨ b·∫±ng 13 =))\n","Cau chua xu ly: :v n√≥i v·∫≠y th√¨ th√† xo√° lu√¥n kh·∫©u vandal ƒëi =))\n","Cau da xu ly: :v n√≥i v·∫≠y th√¨ th√† x√≥a lu√¥n kh·∫©u vandal ƒëi =))\n","Cau chua xu ly: vn m√¨nh view c√≤n ko = 1/10 ngta\n","Cau da xu ly: vn m√¨nh view c√≤n ko = 110 ngta\n","Cau chua xu ly: b·∫£nh ƒë√™√Ω\n","Cau da xu ly: b·∫£nh ƒë·∫øy\n","Cau chua xu ly: ƒë√™m...nh·ª©c nh·ªëi n·ªói ni·ªÅm ri√™ng kh√¥n t·∫£ti·∫øng c√¥n tr√πng r√©o r·∫Øt quy·ªán ti·∫øng m∆∞akh√©p l·∫°i nh√© nh·ªØng t·ªßi h·ªùn u·∫•t h·∫≠ncu·ªôc s·ªëng b·∫•t c√¥ng bi·∫øt m·∫•y cho v·ª´ai ƒë·ªãnh nghƒ©a th·∫ø gian n√†y mu√¥n m·∫∑t\n","Cau da xu ly: ƒë√™m...nh·ª©c nh·ªëi n·ªói ni·ªÅm ri√™ng kh√¥n t·∫£ti·∫øng c√¥n tr√πng r√©o r·∫Øt quy·ªán ti·∫øng m∆∞akh√©p l·∫°i nh√© nh·ªØng t·ªßi h·ªùn u·∫•t h·∫≠ncu·ªôc s·ªëng b·∫•t c√¥ng bi·∫øt m·∫•y cho v∆∞√†i ƒë·ªãnh nghƒ©a th·∫ø gian n√†y mu√¥n m·∫∑t\n","Cau chua xu ly: 2019 aog. 2020 gcs. 2021 rpl. 2022 1/2 aog r·ªìi\n","Cau da xu ly: 2019 aog. 2020 gcs. 2021 rpl. 2022 12 aog r·ªìi\n","Cau chua xu ly: √°m ·∫£nh gi·ªçng ch√≥i tai cu·∫£ lyly qu√°\n","Cau da xu ly: √°m ·∫£nh gi·ªçng ch√≥i tai c·ªßa lyly qu√°\n","Cau chua xu ly: c√°c ch√°u xem gi·∫£i m√† l√∫c n√†o c≈©ng cmt v√¥ vƒÉn ho√° th·∫•t h·ªçc v\n","Cau da xu ly: c√°c ch√°u xem gi·∫£i m√† l√∫c n√†o c≈©ng cmt v√¥ vƒÉn h√≥a th·∫•t h·ªçc v\n","Cau chua xu ly: c√≥ gi·ªèi qua m√† b·∫Øn sao lai ch·ª≠i ng·ª´oi ta\n","Cau da xu ly: c√≥ gi·ªèi qua m√† b·∫Øn sao lai ch·ª≠i ng∆∞√≤i ta\n","Cau chua xu ly: con b·ªç ph·∫£i tu·ª≥ l√∫c m·ªõi nh·∫£y ƒë∆∞·ª£c v√†o. ko n·ªôp m·∫°ng ak.\n","Cau da xu ly: con b·ªç ph·∫£i t√πy l√∫c m·ªõi nh·∫£y ƒë∆∞·ª£c v√†o. ko n·ªôp m·∫°ng ak.\n","Cau chua xu ly: ƒë·∫•u tr∆∞·ªùng danh v·ªçng k·∫øt th√∫c l√¢u r·ªìi m√† ba ://\n","Cau da xu ly: ƒë·∫•u tr∆∞·ªùng danh v·ªçng k·∫øt th√∫c l√¢u r·ªìi m√† ba :\n","Cau chua xu ly: hay q√∫a bro\n","Cau da xu ly: hay qu√° bro\n","Cau chua xu ly: hay to√°\n","Cau da xu ly: hay t√≥a\n","Cau chua xu ly: hq xo√° game lu√¥n ƒëi ƒë·ª´ng c√≥ m√† tham gia tr·∫≠n n√†o nx\n","Cau da xu ly: hq x√≥a game lu√¥n ƒëi ƒë·ª´ng c√≥ m√† tham gia tr·∫≠n n√†o nx\n","Cau chua xu ly: kho√° th√¨ ch∆∞a b·ªã nh∆∞ng nh·ªõ c√°c ac rules g·∫ßn 200 c·ªß , game xo√° vƒ©nh vi·ªÖn\n","Cau da xu ly: kh√≥a th√¨ ch∆∞a b·ªã nh∆∞ng nh·ªõ c√°c ac rules g·∫ßn 200 c·ªß , game x√≥a vƒ©nh vi·ªÖn\n","Cau chua xu ly: m√¢√Ω thg ng√°o r·ªìi √† gow tan r√£ r·ªìi\n","Cau da xu ly: m·∫•y thg ng√°o r·ªìi √† gow tan r√£ r·ªìi\n","Cau chua xu ly: mo√° ƒë√°nh m√† b·ªã b·∫Øt b√†i\n","Cau da xu ly: m√≥a ƒë√°nh m√† b·ªã b·∫Øt b√†i\n","Cau chua xu ly: ngo√†i kia s·∫•m ƒë√°nh ·∫ßm ·∫ßm/ tr·∫ßn b√¨nh ƒë√°nh gi·∫£i t∆∞·ªüng nh·∫ßm lai b√¢ng\n","Cau da xu ly: ngo√†i kia s·∫•m ƒë√°nh ·∫ßm ·∫ßm tr·∫ßn b√¨nh ƒë√°nh gi·∫£i t∆∞·ªüng nh·∫ßm lai b√¢ng\n","Cau chua xu ly: ngx a kho·∫ª h∆°n\n","Cau da xu ly: ngx a kh·ªèe h∆°n\n","Cau chua xu ly: th√°i nh·ªç g√™ q√∫a\n","Cau da xu ly: th√°i nh·ªç g√™ qu√°\n","Cau chua xu ly: to√†n b·ªçn v√¥ hƒÉn ho√° chat\n","Cau da xu ly: to√†n b·ªçn v√¥ hƒÉn h√≥a chat\n","Cau chua xu ly: @hisu tv b·ªØa t√¥i ƒë·∫∑t 1/10 con asimov b√∫ m·∫°nh √¥ng ·∫°\n","Cau da xu ly: @hisu tv b·ªØa t√¥i ƒë·∫∑t 110 con asimov b√∫ m·∫°nh √¥ng ·∫°\n","Cau chua xu ly: 1/0/3 ƒë√∫ng 4 agi lu√¥n :v\n","Cau da xu ly: 103 ƒë√∫ng 4 agi lu√¥n :v\n","Cau chua xu ly: 30/4 √†?\n","Cau da xu ly: 304 √†?\n","Cau chua xu ly: 30/4 mode\n","Cau da xu ly: 304 mode\n","Cau chua xu ly: √† jbo xo√° danh b·∫° ch·∫∑n s·ªë r\n","Cau da xu ly: √† jbo x√≥a danh b·∫° ch·∫∑n s·ªë r\n","Cau chua xu ly: b·∫Øt ƒë√¢u√π r·ªìi\n","Cau da xu ly: b·∫Øt ƒë√¢√πu r·ªìi\n","Cau chua xu ly: batman ho√° ch√≥ r\n","Cau da xu ly: batman h√≥a ch√≥ r\n","Cau chua xu ly: carry 500tr 14/0 awesome\n","Cau da xu ly: carry 500tr 140 awesome\n","Cau chua xu ly: fan b√≤iz say bia\n","Cau da xu ly: fan bo√¨z say bia\n","Cau chua xu ly: l√™n xe awesome/\n","Cau da xu ly: l√™n xe awesome\n","Cau chua xu ly: mo√° n√≠\n","Cau da xu ly: m√≥a n√≠\n","Cau chua xu ly: ng√†i ƒë√°nh 2 tr·∫≠n slark ho√† v·ªën ah ae?\n","Cau da xu ly: ng√†i ƒë√°nh 2 tr·∫≠n slark h√≤a v·ªën ah ae?\n","Cau chua xu ly: omi n√≥ kho·∫ª vcl\n","Cau da xu ly: omi n√≥ kh·ªèe vcl\n","Cau chua xu ly: t·ª± nhi√™n anh wind kho·∫ª lu√¥n :))\n","Cau da xu ly: t·ª± nhi√™n anh wind kh·ªèe lu√¥n :))\n","Cau chua xu ly: ulti l·∫•y music do·∫° n√≥ ch·ª©\n","Cau da xu ly: ulti l·∫•y music d·ªça n√≥ ch·ª©\n","Cau chua xu ly: update w/l @quocthaiak\n","Cau da xu ly: update wl @quocthaiak\n","Cau chua xu ly: vcl h·ªçc ch∆°i th√¨ xem demo 1 th·∫±ng b·∫Øn v·ªã tr√≠ gi·ªëng m√¨nh ch·ª© ƒëi xem gi·∫£i th√¨ h·ªçc ƒë∆∞·ª£c mo·∫π g√¨ awesome\n","Cau da xu ly: vcl h·ªçc ch∆°i th√¨ xem demo 1 th·∫±ng b·∫Øn v·ªã tr√≠ gi·ªëng m√¨nh ch·ª© ƒëi xem gi·∫£i th√¨ h·ªçc ƒë∆∞·ª£c m·ªçe g√¨ awesome\n","Cau chua xu ly: vcl t·ª± hu·ª∑ ki·ªÉu g√¨ th·∫ø :))\n","Cau da xu ly: vcl t·ª± h·ªßy ki·ªÉu g√¨ th·∫ø :))\n","Cau chua xu ly: w·ª∑ 3/3/3 awesome\n","Cau da xu ly: w·ª∑ 333 awesome\n","Cau chua xu ly: b·ªô 3 hu·ª∑ di·ªát :))))))) b√° ƒë·∫ßn\n","Cau da xu ly: b·ªô 3 h·ªßy di·ªát :))))))) b√° ƒë·∫ßn\n","Cau chua xu ly: th·∫Øng ƒëc v√°n g·ª° ho√† m√† c·ª© nh∆∞ ch·∫øt ƒëi s·ªëng l·∫°i g√°y gh√™ th·∫≠t :))\n","Cau da xu ly: th·∫Øng ƒëc v√°n g·ª° h√≤a m√† c·ª© nh∆∞ ch·∫øt ƒëi s·ªëng l·∫°i g√°y gh√™ th·∫≠t :))\n","Cau chua xu ly: r·ªìif sao team m ch·ªãu ƒë∆∞·ª£c\n","Cau da xu ly: r√¥√¨f sao team m ch·ªãu ƒë∆∞·ª£c\n","Cau chua xu ly: po5 ƒëi mo√†\n","Cau da xu ly: po5 ƒëi m√≤a\n","Cau chua xu ly: ƒë·ªì ho·∫° msi ƒë·ªôc l·∫° qu√°\n","Cau da xu ly: ƒë·ªì h·ªça msi ƒë·ªôc l·∫° qu√°\n","Cau chua xu ly: ƒë·ªãt m·∫π ch√∫ng m√†y b√°n ƒë·ªô l·ªô l·∫Øm m·∫•y con ch√≥ n√†y /)))\n","Cau da xu ly: ƒë·ªãt m·∫π ch√∫ng m√†y b√°n ƒë·ªô l·ªô l·∫Øm m·∫•y con ch√≥ n√†y )))\n","Cau chua xu ly: g2 ƒë√°nh con c·∫∑c g√¨ v·∫≠y ???/\n","Cau da xu ly: g2 ƒë√°nh con c·∫∑c g√¨ v·∫≠y ???\n","Cau chua xu ly: ??? b·∫Øt ƒëc ad r·ªìi m√† thua ƒëc.? atroc n√≥ kho·∫ª hay sao ·∫•y kyen n·ªØa kho·∫ª\n","Cau da xu ly: ??? b·∫Øt ƒëc ad r·ªìi m√† thua ƒëc.? atroc n√≥ kh·ªèe hay sao ·∫•y kyen n·ªØa kh·ªèe\n","Cau chua xu ly: ·ªëi d·ªìi √¥i ai ƒë·ªô e t√¥i m√† 1/6 th·∫ø kia\n","Cau da xu ly: ·ªëi d·ªìi √¥i ai ƒë·ªô e t√¥i m√† 16 th·∫ø kia\n","Cau chua xu ly: r·ªìng ho√° k·ªπ kh√°c bi·ªát\n","Cau da xu ly: r·ªìng h√≥a k·ªπ kh√°c bi·ªát\n","Cau chua xu ly: ng√†i pyos.. √† nh·∫ßm mark ƒëo√°n gam, blg v·ªõi ƒë·ªôi m·∫°nh nh·∫•t b·∫£ng c√≤n l·∫°i s·∫Ω v√†o; ng√†i ƒë√∫ng 2/3 r·ªìi :((\n","Cau da xu ly: ng√†i pyos.. √† nh·∫ßm mark ƒëo√°n gam, blg v·ªõi ƒë·ªôi m·∫°nh nh·∫•t b·∫£ng c√≤n l·∫°i s·∫Ω v√†o; ng√†i ƒë√∫ng 23 r·ªìi :((\n","Cau chua xu ly: 7 c·ªè b·ªè b√≥ng ƒë√° ng∆∞·ªùi, v·∫≠n ƒë·ªông vi√™n ƒë·∫•u v·∫≠t chuy√™n nghi·ªáp, hung th·∫ßn ƒë·∫≠p ƒëi·ªán tho·∫°i fan nh√≠, k·∫ª hu·ª∑ di·ªát gi·∫£i l·∫°c ƒë√† h·∫°ng 66.\n","Cau da xu ly: 7 c·ªè b·ªè b√≥ng ƒë√° ng∆∞·ªùi, v·∫≠n ƒë·ªông vi√™n ƒë·∫•u v·∫≠t chuy√™n nghi·ªáp, hung th·∫ßn ƒë·∫≠p ƒëi·ªán tho·∫°i fan nh√≠, k·∫ª h·ªßy di·ªát gi·∫£i l·∫°c ƒë√† h·∫°ng 66.\n","Cau chua xu ly: ti·∫øn ho√° 4 chi√™u =)))\n","Cau da xu ly: ti·∫øn h√≥a 4 chi√™u =)))\n","Cau chua xu ly: 0/0/0 m√† c√≥ ti·ªÅn th∆∞·ªüng\n","Cau da xu ly: 000 m√† c√≥ ti·ªÅn th∆∞·ªüng\n"]}]},{"cell_type":"code","source":["dataset['Comment'][6976]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"VqgtqjiN85bH","executionInfo":{"status":"ok","timestamp":1688454904282,"user_tz":-420,"elapsed":467,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"288de53d-4471-45a6-8d72-88aed6288222"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ƒë√≠u ra dao'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWmwXYNGt_44","outputId":"65850c33-3304-4ce1-e32b-dd12333782e2","executionInfo":{"status":"ok","timestamp":1688454904767,"user_tz":-420,"elapsed":2,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["print(link_count)\n","print(unicode_count)\n","print(dau_count)\n","print(lower_count)\n","print(noneed_count)\n","print(space_count)\n","# print(len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","0\n","55\n","2171\n","458\n","96\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"3BgffCFZe8ka","outputId":"da8623b9-e416-4e83-8a60-57fd3396d7ff","executionInfo":{"status":"ok","timestamp":1688454906520,"user_tz":-420,"elapsed":5,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["text = u'This is a smiley face üòÇ'\n","text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'This is a smiley face üòÇ'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"cFnYCXtDM7tJ","executionInfo":{"status":"ok","timestamp":1688454245640,"user_tz":-420,"elapsed":6831,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc922e95-a404-4480-f10a-3201cab8440f"},"source":["!pip install spacymoji"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting spacymoji\n","  Downloading spacymoji-3.1.0-py2.py3-none-any.whl (8.5 kB)\n","Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacymoji) (3.5.3)\n","Collecting emoji<3.0,>=2.0 (from spacymoji)\n","  Downloading emoji-2.6.0.tar.gz (356 kB)\n","\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/356.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m348.2/356.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m356.6/356.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (8.1.10)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.10.9)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacymoji) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacymoji) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacymoji) (2.1.3)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.6.0-py2.py3-none-any.whl size=351311 sha256=fcd390083e63411f45b37d36688d5fb0782564c3e65218650dd3baf252b6b0bc\n","  Stored in directory: /root/.cache/pip/wheels/ea/0b/64/114bc939d0083621aa41521e21be246c888260b8aa21e6c1ad\n","Successfully built emoji\n","Installing collected packages: emoji, spacymoji\n","Successfully installed emoji-2.6.0 spacymoji-3.1.0\n"]}]},{"cell_type":"code","metadata":{"id":"kPj5H-HXN5ht","executionInfo":{"status":"ok","timestamp":1688454273367,"user_tz":-420,"elapsed":27732,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"056478d9-50fc-479f-fd78-d937c9959c40"},"source":["!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-04 07:04:13.817482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-04 07:04:15.861318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7Owkp-ZNMq6","outputId":"6d5672fb-7115-4a97-e9af-3a1eeac5b6ec","executionInfo":{"status":"ok","timestamp":1688454912354,"user_tz":-420,"elapsed":1267,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["import spacy\n","import en_core_web_sm\n","from spacymoji import Emoji\n","\n","nlp = en_core_web_sm.load()\n","emoji = Emoji(nlp)\n","nlp.add_pipe('emoji', first=True)\n","\n","doc = nlp(\"sale hay sela :))\")\n","print([token.text for token in doc])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['sale', 'hay', 'sela', ':))']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8AljlN8R4F6","outputId":"59c3ea10-72a5-445a-ea2c-30b6f1e109a4","executionInfo":{"status":"ok","timestamp":1688454914073,"user_tz":-420,"elapsed":6,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["doc = nlp(\"sale hay sela :)))))))))))))) th·∫≠t v√£i l·ªìn ha\")\n","print([token.text for token in doc])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['sale', 'hay', 'sela', ':)))', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'th·∫≠t', 'v√£i', 'l·ªìn', 'ha']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HpooafddW_s0","outputId":"48d8ec46-763b-4487-8e4b-d14ff61ec032","executionInfo":{"status":"ok","timestamp":1688454915285,"user_tz":-420,"elapsed":3,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["doc[6]._.is_emoji"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["#!pip install python-rdrsegmenter\n"],"metadata":{"id":"7RTI-eg9BjxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvqmOXGZPd2t"},"source":["word_segmented_text = rdrsegmenter.tokenize(\"sale hay sela :D th·∫≠t v√£i l·ªìn ha üòÇüòÇüòÇüòÇüòÇ\")\n","word_segmented_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aK7GdrRkT3yl","outputId":"9144480f-f776-431e-daaa-8619b007d649","executionInfo":{"status":"ok","timestamp":1688454924267,"user_tz":-420,"elapsed":4866,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["!pip install demoji"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: demoji in /usr/local/lib/python3.10/dist-packages (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdLqiFAjTqN-","outputId":"8ca8723f-f7c7-4b0d-886e-904b6a1310ad","executionInfo":{"status":"ok","timestamp":1688454924270,"user_tz":-420,"elapsed":29,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["import demoji\n","demoji.download_codes()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-77-eb011a9810ad>:2: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n","  demoji.download_codes()\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"CsncdHBOUFSP","outputId":"b6036115-535c-41df-9b15-79294d7ff611","executionInfo":{"status":"ok","timestamp":1688454924273,"user_tz":-420,"elapsed":28,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["text = \"sale hay sela :D th·∫≠t v√£i l·ªìn ha üòÇüòÇüòÇüòÇüòÇ\"\n","re.sub(r'([A-Z])\\1+', lambda m: m.group(1), text, flags = re.IGNORECASE)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'sale hay sela :D th·∫≠t v√£i l·ªìn ha üòÇüòÇüòÇüòÇüòÇ'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":78}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4QrjEo4U86u","outputId":"ac9d14b0-4947-434c-b8e5-c735f52658af","executionInfo":{"status":"ok","timestamp":1688454924274,"user_tz":-420,"elapsed":27,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["demoji.findall(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'üòÇ': 'face with tears of joy'}"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["# Test\n","text = '       ti√™n tri n√≥i         qu·∫±n q√πeeeee hay vcllll :))       ) '\n","phase1_text = text_preprocess(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4T9ipq7MFNA","executionInfo":{"status":"ok","timestamp":1688454924274,"user_tz":-420,"elapsed":24,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"d31d6940-5153-49b9-fd82-b25b5cd05f0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cau chua xu ly: ti√™n tri n√≥i qu·∫±n q√πe hay vcl :)) )\n","Cau da xu ly: ti√™n tri n√≥i qu·∫±n qu√® hay vcl :)) )\n"]}]},{"cell_type":"code","source":["phase1_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"63imBtbYMcoR","executionInfo":{"status":"ok","timestamp":1688454924274,"user_tz":-420,"elapsed":18,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"f90012b6-da64-435f-96b9-aa2124184e87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ti√™n tri n√≥i qu·∫±n qu√® hay vcl :)) )'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","metadata":{"id":"PP_RDw3m1Xke"},"source":["# Word Tokenize"]},{"cell_type":"code","metadata":{"id":"fx7gu5UHL7oJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"398d99a1-846c-4115-a2bd-7a180524dd8f","executionInfo":{"status":"ok","timestamp":1688453709769,"user_tz":-420,"elapsed":8035,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["# Word segmenter\n","!pip3 install vncorenlp\n","\n","# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n","!mkdir -p vncorenlp/models/wordsegmenter\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","!mv VnCoreNLP-1.1.1.jar vncorenlp/\n","!mv vi-vocab vncorenlp/models/wordsegmenter/\n","!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vncorenlp\n","  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=ff45ffa2372ffc5d4e53b43f0e99173edc0931a2e0169487ad1a281bd9faba00\n","  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n","Successfully built vncorenlp\n","Installing collected packages: vncorenlp\n","Successfully installed vncorenlp-1.0.3\n","--2023-07-04 06:55:07--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 27412575 (26M) [application/octet-stream]\n","Saving to: ‚ÄòVnCoreNLP-1.1.1.jar‚Äô\n","\n","VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   168MB/s    in 0.2s    \n","\n","2023-07-04 06:55:08 (168 MB/s) - ‚ÄòVnCoreNLP-1.1.1.jar‚Äô saved [27412575/27412575]\n","\n","--2023-07-04 06:55:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 526544 (514K) [application/octet-stream]\n","Saving to: ‚Äòvi-vocab‚Äô\n","\n","vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.05s   \n","\n","2023-07-04 06:55:08 (10.1 MB/s) - ‚Äòvi-vocab‚Äô saved [526544/526544]\n","\n","--2023-07-04 06:55:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 128508 (125K) [text/plain]\n","Saving to: ‚Äòwordsegmenter.rdr‚Äô\n","\n","wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n","\n","2023-07-04 06:55:08 (5.54 MB/s) - ‚Äòwordsegmenter.rdr‚Äô saved [128508/128508]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"R_0dQ9J7L8WJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fdac1c05-a77f-435d-ecca-c51a96ab7aaa","executionInfo":{"status":"ok","timestamp":1688453720257,"user_tz":-420,"elapsed":6274,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["from vncorenlp import VnCoreNLP\n","rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","\n","text = \"hello, h√≤a thu·∫≠n\"\n","\n","word_segmented_text = rdrsegmenter.tokenize(text)\n","print(type(word_segmented_text))\n","print(word_segmented_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","[['hello', ',', 'ho√†_thu·∫≠n']]\n"]}]},{"cell_type":"markdown","metadata":{"id":"4Y9c4CH6P-JQ"},"source":["##Teencode"]},{"cell_type":"code","metadata":{"id":"JYirdMTpWKB_"},"source":["import pandas as pd\n","teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXnHXdThfxoD"},"source":["teencode_list = teencode_df['teencode'].to_list()\n","map_list = teencode_df['map'].to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFDlRrGa1yYx"},"source":["def searchTeencode(word):\n","  try:\n","    global teencode_count\n","    index = teencode_list.index(word)\n","    map_word = map_list[index]\n","    teencode_count += 1\n","    return map_word\n","  except:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8j_LvroNVD6b"},"source":["##Stopwords"]},{"cell_type":"code","source":["x=2\n","if (x==0)!= (x==1):\n","  print(1)"],"metadata":{"id":"8gowXKnnQgYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrxeq8rCVMdm"},"source":["# stopword = ['b·ªã', 'b·ªüi', 'c·∫£', 'c√°c', 'c√°i', 'c·∫ßn', 'c√†ng', 'ch·ªâ', 'chi·∫øc', 'cho', 'ch·ª©', 'ch∆∞a', 'chuy·ªán', 'c√≥', 'c√≥_th·ªÉ', 'c·ª©',\n","#             'c·ªßa', 'c√πng', 'c≈©ng', 'ƒë√£', 'ƒëang', 'ƒë√¢y', 'ƒë·ªÉ', 'ƒë·∫øn_n·ªói', 'ƒë·ªÅu', 'ƒëi·ªÅu', 'do', 'ƒë√≥', 'ƒë∆∞·ª£c', 'd∆∞·ªõi', 'g√¨', 'khi',\n","#             'kh√¥ng', 'l√†', 'l·∫°i', 'l√™n', 'l√∫c', 'm√†', 'm·ªói', 'm·ªôt_c√°ch', 'n√†y', 'n√™n', 'n·∫øu', 'ngay', 'nhi·ªÅu', 'nh∆∞', 'nh∆∞ng',\n","#             'nh·ªØng', 'n∆°i', 'n·ªØa', 'ph·∫£i', 'qua', 'ra', 'r·∫±ng', 'r·∫•t', 'r·ªìi', 'sau', 's·∫Ω', 'so', 's·ª±', 't·∫°i', 'theo', 'th√¨', 'tr√™n',\n","#             'tr∆∞·ªõc', 't·ª´', 't·ª´ng', 'v√†', 'v·∫´n', 'v√†o', 'v·∫≠y', 'v√¨', 'vi·ªác', 'v·ªõi', 'v·ª´a']\n","\n","STOPWORDS = '/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/vietnamese-stopwords-dash.txt'\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stopword = []\n","    for line in ins:\n","        stopword.append(line.strip('\\n'))\n","\n","def remove_stopwords(line):\n","    global stopword_count\n","    words = []\n","    for word in line:\n","        if word not in stopword:\n","            words.append(word)\n","        if word in stopword:\n","            stopword_count += 1\n","    return words\n","\n","# with open(STOPWORDS, \"r\") as ins:\n","#     stop_words = []\n","#     for line in ins:\n","#         stop_words.append(line.strip('\\n'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zuc_i7KZfGFj"},"source":["##Combine teencode + stopwords"]},{"cell_type":"code","metadata":{"id":"bjZWbRZFfFfx"},"source":["#DE STOPWORD_TEENCODE BY VNCORENLP\n","stopword_count = 0\n","teencode_count =0\n","def stopWords_Teencode(sentence):\n","  lenn = 0\n","  sentence = str(sentence)\n","  #Tokenize\n","  nestList_tokens = rdrsegmenter.tokenize(sentence)\n","  for tokens_idx, text_tokens in enumerate(nestList_tokens):\n","    #Teencode\n","    lenn += len(text_tokens)\n","    for idx, word in enumerate(text_tokens):\n","      deteencoded = searchTeencode(word)\n","      if(deteencoded != None):\n","        text_tokens[idx] = deteencoded\n","    nestList_tokens[tokens_idx] = text_tokens\n","\n","  # deteencode_sentence = (\" \").join(nestList_tokens)\n","\n","  #Stopwords\n","  flat_list = [item for sublist in nestList_tokens for item in sublist]\n","\n","  List_without_sw = remove_stopwords(flat_list)\n","\n","  #Detokenize\n","  detokens = MosesDetokenizer().detokenize(List_without_sw, return_str=True)\n","\n","  return detokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ur6RzLdjM92E"},"source":["sent = 'ti√™n tri n√≥i qu·∫±n qu√® hay clm :)) ) '"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"KOTuF6eLMu9a","outputId":"efd0d939-af94-435f-d77e-b7f6a3b8f317","executionInfo":{"status":"ok","timestamp":1688453779892,"user_tz":-420,"elapsed":10,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["stopWords_Teencode(sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ti√™n_tri qu·∫±n qu√® c√°i l·ªìn m√°:)))'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"PU3MAi7kgZRd"},"source":["stopword_count = 0\n","teencode_count =0\n","dataset['Comment'] = dataset['Comment'].apply(lambda x:stopWords_Teencode(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNcqaGM4O7y4","outputId":"fcb03595-b9d4-4a3c-ca35-d1d15f60545e","executionInfo":{"status":"ok","timestamp":1688453826755,"user_tz":-420,"elapsed":308,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["stopword_count, teencode_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(17480, 2793)"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"HFm-1YpqU8ez"},"source":["#WORD TOKENIZE BY UNDERTHESEA"]},{"cell_type":"code","metadata":{"id":"cqrwpmOiVtNe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688454324213,"user_tz":-420,"elapsed":18097,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"f5163eba-0dc8-47c2-dbc2-cf25dce98fa3"},"source":["!pip install underthesea\n","\n","from underthesea import word_tokenize"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting underthesea\n","  Downloading underthesea-6.3.0-py3-none-any.whl (19.2 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.3)\n","Collecting python-crfsuite>=0.9.6 (from underthesea)\n","  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n","Collecting underthesea-core==1.0.4 (from underthesea)\n","  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n","Installing collected packages: underthesea-core, python-crfsuite, underthesea\n","Successfully installed python-crfsuite-0.9.9 underthesea-6.3.0 underthesea-core-1.0.4\n"]}]},{"cell_type":"markdown","metadata":{"id":"Fr2T8sEbVqHN"},"source":["##Teencode"]},{"cell_type":"code","metadata":{"id":"ERALhVzTVyy6"},"source":["import pandas as pd\n","teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)\n","teencode_list = teencode_df['teencode'].to_list()\n","map_list = teencode_df['map'].to_list()\n","def searchTeencode(word):\n","  try:\n","    global teencode_count\n","    index = teencode_list.index(word)\n","    map_word = map_list[index]\n","    teencode_count += 1\n","    return map_word\n","  except:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ihpm8p8FV1WC"},"source":["##Stopwords"]},{"cell_type":"code","metadata":{"id":"GMjO3xjuVywx"},"source":["# stopword = ['b·ªã', 'b·ªüi', 'c·∫£', 'c√°c', 'c√°i', 'c·∫ßn', 'c√†ng', 'ch·ªâ', 'chi·∫øc', 'cho', 'ch·ª©', 'ch∆∞a', 'chuy·ªán', 'c√≥', 'c√≥_th·ªÉ', 'c·ª©',\n","#             'c·ªßa', 'c√πng', 'c≈©ng', 'ƒë√£', 'ƒëang', 'ƒë√¢y', 'ƒë·ªÉ', 'ƒë·∫øn_n·ªói', 'ƒë·ªÅu', 'ƒëi·ªÅu', 'do', 'ƒë√≥', 'ƒë∆∞·ª£c', 'd∆∞·ªõi', 'g√¨', 'khi',\n","#             'kh√¥ng', 'l√†', 'l·∫°i', 'l√™n', 'l√∫c', 'm√†', 'm·ªói', 'm·ªôt_c√°ch', 'n√†y', 'n√™n', 'n·∫øu', 'ngay', 'nhi·ªÅu', 'nh∆∞', 'nh∆∞ng',\n","#             'nh·ªØng', 'n∆°i', 'n·ªØa', 'ph·∫£i', 'qua', 'ra', 'r·∫±ng', 'r·∫•t', 'r·ªìi', 'sau', 's·∫Ω', 'so', 's·ª±', 't·∫°i', 'theo', 'th√¨', 'tr√™n',\n","#             'tr∆∞·ªõc', 't·ª´', 't·ª´ng', 'v√†', 'v·∫´n', 'v√†o', 'v·∫≠y', 'v√¨', 'vi·ªác', 'v·ªõi', 'v·ª´a']\n","\n","STOPWORDS = '/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/vietnamese-stopwords-dash.txt'\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stopword = []\n","    for line in ins:\n","        stopword.append(line.strip('\\n'))\n","\n","def remove_stopwords(line):\n","    global stopword_count\n","    words = []\n","    for word in line.strip().split():\n","        if word not in stopword:\n","            words.append(word)\n","        if word in stopword:\n","            stopword_count += 1\n","    return ' '.join(words)\n","\n","# with open(STOPWORDS, \"r\") as ins:\n","#     stop_words = []\n","#     for line in ins:\n","#         stop_words.append(line.strip('\\n'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL3znVvBV6dX"},"source":["##COMBINE TEEEN AND STOPWORD"]},{"cell_type":"code","metadata":{"id":"9RAG6a-OVytU"},"source":["stopword_count = 0\n","teencode_count =0\n","def stopWords_Teencode(sentence):\n","  lenn = 0\n","  sentence = str(sentence)\n","  #Tokenize\n","  List_tokens = word_tokenize(sentence,format='text')\n","  List_tokens = word_tokenize(List_tokens)\n","\n","  #Teencode\n","  for tokens_idx, text_tokens in enumerate(List_tokens):\n","    deteencoded = searchTeencode(text_tokens)\n","    if (deteencoded != None):\n","        List_tokens[tokens_idx] = deteencoded\n","\n","  deteencode_sentence = (\" \").join(List_tokens)\n","\n","  #Stopwords\n","  tokens_without_sw = remove_stopwords(deteencode_sentence)\n","\n","  return tokens_without_sw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIJwn4gOXEAy"},"source":["sent = 'ho√†ng gia ph√∫ ƒë·∫πp trai vl, ƒë√©o th·ªÉ tin ƒë∆∞·ª£c nh∆∞ng m√† c√≤n ngu n·ªØa, haha. Con c·∫∑c, ƒëjt m·∫π, thg ƒë√†n b√† b√™ ƒë√™ ch√∫a'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zp8n5gJMXq8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688454367962,"user_tz":-420,"elapsed":445,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}},"outputId":"e7ac01ab-93f1-481f-e946-14c1f8b699fb"},"source":["word_tokenize(sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ho√†ng gia',\n"," 'ph√∫',\n"," 'ƒë·∫πp trai',\n"," 'vl',\n"," ',',\n"," 'ƒë√©o th·ªÉ',\n"," 'tin',\n"," 'ƒë∆∞·ª£c',\n"," 'nh∆∞ng',\n"," 'm√†',\n"," 'c√≤n',\n"," 'ngu',\n"," 'n·ªØa',\n"," ',',\n"," 'haha',\n"," '. Con',\n"," 'c·∫∑c',\n"," ',',\n"," 'ƒëjt',\n"," 'm·∫π',\n"," ',',\n"," 'thg',\n"," 'ƒë√†n b√†',\n"," 'b√™',\n"," 'ƒë√™',\n"," 'ch√∫a']"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"_6wPEg7aX3Tn"},"source":["#rdrsegmenter.tokenize(sent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"nju9p84hXG1J","outputId":"7683748a-1101-4243-8d54-1d6901b20e12","executionInfo":{"status":"ok","timestamp":1688454403224,"user_tz":-420,"elapsed":287,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["stopWords_Teencode(sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ho√†ng_gia ph√∫ ƒë·∫πp_trai v√£i l·ªìn , ƒë√©o_th·ªÉ ngu , haha . _Con c·∫∑c , ƒëjt m·∫π , thg ƒë√†n_b√† b√™ ƒë√™ ch√∫a'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"StrQPCecWClh"},"source":["##DO IT"]},{"cell_type":"code","metadata":{"id":"28GJXoaHVyqt"},"source":["stopword_count = 0\n","teencode_count =0\n","dataset['Comment'] = dataset['Comment'].apply(lambda x:stopWords_Teencode(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pe7X1plSVynU","outputId":"b88bc7ca-5fef-44ac-d7f5-27bd5651ee4a","executionInfo":{"status":"ok","timestamp":1688454422086,"user_tz":-420,"elapsed":313,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["teencode_count, stopword_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2854, 18529)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"L_RNtGiqhCTJ"},"source":["test1 = test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbnhXpLLX55E"},"source":["# Word Tokenize NLTK"]},{"cell_type":"code","metadata":{"id":"kiOQ9GNjX55Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f3ae051-3476-406b-cd21-d1472db7740a","executionInfo":{"status":"ok","timestamp":1688454986929,"user_tz":-420,"elapsed":57302,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["!sudo pip install nltk\n","import nltk\n","nltk.download('all')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPXj9xBBX55R","outputId":"22ade6cd-926b-41bf-9b57-22ef80ff1fa2","executionInfo":{"status":"ok","timestamp":1688454994289,"user_tz":-420,"elapsed":4,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","\n","sent = 'ho√†ng gia ph√∫ ƒë·∫πp trai vl, ƒë√©o th·ªÉ tin ƒë∆∞·ª£c nh∆∞ng m√† c√≤n ngu n·ªØa, haha. Con c·∫∑c, ƒëjt m·∫π, thg ƒë√†n b√† b√™ ƒë√™ ch√∫a'\n","print(word_tokenize(sent))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['ho√†ng', 'gia', 'ph√∫', 'ƒë·∫πp', 'trai', 'vl', ',', 'ƒë√©o', 'th·ªÉ', 'tin', 'ƒë∆∞·ª£c', 'nh∆∞ng', 'm√†', 'c√≤n', 'ngu', 'n·ªØa', ',', 'haha', '.', 'Con', 'c·∫∑c', ',', 'ƒëjt', 'm·∫π', ',', 'thg', 'ƒë√†n', 'b√†', 'b√™', 'ƒë√™', 'ch√∫a']\n"]}]},{"cell_type":"markdown","metadata":{"id":"V6XcMZxuX55S"},"source":["##Teencode"]},{"cell_type":"code","metadata":{"id":"UIJ6ZG7GX55V"},"source":["import pandas as pd\n","teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHSWaanAX55V"},"source":["teencode_list = teencode_df['teencode'].to_list()\n","map_list = teencode_df['map'].to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-0TgZqOX55W"},"source":["def searchTeencode(word):\n","  try:\n","    global teencode_count\n","    index = teencode_list.index(word)\n","    map_word = map_list[index]\n","    teencode_count += 1\n","    return map_word\n","  except:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lysu4TNoX55W"},"source":["##Stopwords"]},{"cell_type":"code","metadata":{"id":"aLlFC4WkX55W"},"source":["# stopword = ['b·ªã', 'b·ªüi', 'c·∫£', 'c√°c', 'c√°i', 'c·∫ßn', 'c√†ng', 'ch·ªâ', 'chi·∫øc', 'cho', 'ch·ª©', 'ch∆∞a', 'chuy·ªán', 'c√≥', 'c√≥_th·ªÉ', 'c·ª©',\n","#             'c·ªßa', 'c√πng', 'c≈©ng', 'ƒë√£', 'ƒëang', 'ƒë√¢y', 'ƒë·ªÉ', 'ƒë·∫øn_n·ªói', 'ƒë·ªÅu', 'ƒëi·ªÅu', 'do', 'ƒë√≥', 'ƒë∆∞·ª£c', 'd∆∞·ªõi', 'g√¨', 'khi',\n","#             'kh√¥ng', 'l√†', 'l·∫°i', 'l√™n', 'l√∫c', 'm√†', 'm·ªói', 'm·ªôt_c√°ch', 'n√†y', 'n√™n', 'n·∫øu', 'ngay', 'nhi·ªÅu', 'nh∆∞', 'nh∆∞ng',\n","#             'nh·ªØng', 'n∆°i', 'n·ªØa', 'ph·∫£i', 'qua', 'ra', 'r·∫±ng', 'r·∫•t', 'r·ªìi', 'sau', 's·∫Ω', 'so', 's·ª±', 't·∫°i', 'theo', 'th√¨', 'tr√™n',\n","#             'tr∆∞·ªõc', 't·ª´', 't·ª´ng', 'v√†', 'v·∫´n', 'v√†o', 'v·∫≠y', 'v√¨', 'vi·ªác', 'v·ªõi', 'v·ª´a']\n","\n","STOPWORDS = '/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/vietnamese_stopwords.txt'\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stopword = []\n","    for line in ins:\n","        stopword.append(line.strip('\\n'))\n","\n","def remove_stopwords(line):\n","    global stopword_count\n","    words = []\n","    for word in line:\n","        if word not in stopword:\n","            words.append(word)\n","        if word in stopword:\n","            stopword_count += 1\n","    return words\n","\n","# with open(STOPWORDS, \"r\") as ins:\n","#     stop_words = []\n","#     for line in ins:\n","#         stop_words.append(line.strip('\\n'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJNM3QlPX55X"},"source":["##Combine teencode + stopwords"]},{"cell_type":"code","metadata":{"id":"b9QzShKHfQQn"},"source":["stopword_count = 0\n","teencode_count =0\n","def stopWords_Teencode(sentence):\n","  lenn = 0\n","  sentence = str(sentence)\n","  #Tokenize\n","  List_tokens = word_tokenize(sentence)\n","\n","  #Teencode\n","  for tokens_idx, text_tokens in enumerate(List_tokens):\n","    deteencoded = searchTeencode(text_tokens)\n","    if (deteencoded != None):\n","        List_tokens[tokens_idx] = deteencoded\n","\n","  #Stopwords\n","  list_without_sw = remove_stopwords(List_tokens)\n","\n","  return TreebankWordDetokenizer().detokenize(list_without_sw)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3Da10OvX55X"},"source":["sent = 'ho√†ng gia ph√∫ ƒë·∫πp trai vl, ƒë√©o th·ªÉ tin ƒë∆∞·ª£c nh∆∞ng m√† c√≤n ngu n·ªØa, haha. Con c·∫∑c, ƒëjt m·∫π, thg ƒë√†n b√† b√™ ƒë√™ ch√∫a'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"uRMH7iWoX55Y","outputId":"bad0ec24-0241-42db-da2f-1e544a52a09b","executionInfo":{"status":"ok","timestamp":1688455048715,"user_tz":-420,"elapsed":8,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["stopWords_Teencode(sent)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ho√†ng gia ph√∫ ƒë·∫πp trai v√£i l·ªìn, ƒë√©o th·ªÉ ngu, haha . Con c·∫∑c, ƒëjt m·∫π, thg ƒë√†n b√™ ƒë√™ ch√∫a'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"uaFYbQHMHnrB"},"source":["#test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNyvrRjUX55Y"},"source":["stopword_count = 0\n","teencode_count =0\n","dataset['Comment'] = dataset['Comment'].apply(lambda x:stopWords_Teencode(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkEe5boKX55Z","outputId":"7d7cae3c-a2f3-41c4-c0be-07e2c0ef0faf","executionInfo":{"status":"ok","timestamp":1688455084263,"user_tz":-420,"elapsed":292,"user":{"displayName":"Anh Ph·∫°m Th·ªã Tr√¢m","userId":"15461037140885299192"}}},"source":["stopword_count, teencode_count"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(18569, 2797)"]},"metadata":{},"execution_count":93}]},{"cell_type":"markdown","metadata":{"id":"VF__6Sobo7tz"},"source":["# EXPORT"]},{"cell_type":"code","metadata":{"id":"BbfP4n2rhP37"},"source":["dataset.to_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/NLTK(Phase2).csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kaLFKBDg69D"},"source":[],"execution_count":null,"outputs":[]}]}